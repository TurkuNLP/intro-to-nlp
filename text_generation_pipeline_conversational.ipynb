{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/text_generation_pipeline_conversational.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation example with conversational model and model quantization\n",
        "\n",
        "This is a brief example of how to run text generation with a conversational (chat, instruct) causal language model, `pipeline`, and model quantization.\n",
        "\n",
        "[Model quantization](https://huggingface.co/docs/accelerate/usage_guides/quantization) is a technique to reduce the computational and memory costs of running inference by representing the weights and activations with low-precision data types like 8-bit integer (int8) instead of the usual 32-bit floating point (float32)."
      ],
      "metadata": {
        "id": "tIQ1s96UCcJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet transformers accelerate bitsandbytes>0.37.0"
      ],
      "metadata": {
        "id": "4fUBJmXHCHw-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the `AutoTokenizer`, `AutoModelForCausalLM`, and `pipeline` classes. The first two support loading tokenizers and generative models from the [Hugging Face repository](https://huggingface.co/models), and the last wraps a tokenizer and a model for convenience. `BitsAndBytesConfig` is used for model quantization."
      ],
      "metadata": {
        "id": "5ZRNZgRJCt6Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jwyK005xCFSF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a generative model and its tokenizer. You can substitute any other generative model name here, but note that Colab may have issues running larger models.\n",
        "\n",
        "`google/gemma-3-1b-it` model needs authentication, `userdata.get('my_access_token')` reads my secret authentication token I created for my hf account."
      ],
      "metadata": {
        "id": "6QJPDe3ZC_sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = 'google/gemma-3-1b-it'\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side='left', token=userdata.get('my_access_token'))\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, token=userdata.get('my_access_token'), device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "wqTxn_QaCNjZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate a text generation pipeline using the tokenizer and model."
      ],
      "metadata": {
        "id": "9ADWWb77e1sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pipe = pipeline(\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "0IIJzNrEe5qx",
        "outputId": "96f7bf32-611b-4bda-d09d-cc467aae108e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now call the pipeline with a text prompt; it will take care of tokenizing, encoding, generation, and decoding:"
      ],
      "metadata": {
        "id": "eAohNr1ciwaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = [{\"role\": \"user\", \"content\": \"What is the capital of Finland?\"}] # we have only one message here, but we could also have a conversation with multiple user/assistant turns as a starting point\n",
        "\n",
        "output = pipe(conversation, max_new_tokens=50)\n",
        "\n",
        "for c in output[0][\"generated_text\"]:\n",
        "  print(f\"{c['role']}: {c['content']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWcOJkiKi5vr",
        "outputId": "ffc06012-f93b-48d0-d277-d75ed7acad16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: What is the capital of Finland?\n",
            "assistant: The capital of Finland is **Helsinki**.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also call the pipeline with any arguments that the model `generate` function supports. For details on text generation using `transformers`, see e.g. [this tutorial](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "Example with sampling and a high `temperature` parameter to generate more chaotic output:"
      ],
      "metadata": {
        "id": "YROp3hyikXPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = pipe(conversation,\n",
        "              do_sample=True,\n",
        "              temperature=2.0,\n",
        "              max_new_tokens=50)\n",
        "\n",
        "for c in output[0][\"generated_text\"]:\n",
        "  print(f\"{c['role']}: {c['content']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22QjXE88jkim",
        "outputId": "362fff48-fca4-481d-aac5-58f6122fe589"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: What is the capital of Finland?\n",
            "assistant: The capital of Finland is **Stockholm**.\n",
            "\n",
            "Interestingly, this is a long and somewhat convoluted story! Finland was formerly part of Sweden, and as such, the capital of Sweden was Stockholm. In the 16th century, the Finnish Tsar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print memory usage\n",
        "\n",
        "import sys\n",
        "\n",
        "def report_memory_usage(message, out=sys.stderr):\n",
        "    print(f'max memory allocation {message}:', file=out)\n",
        "    total = 0\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        mem = torch.cuda.max_memory_allocated(i)\n",
        "        print(f'  cuda:{i}: {mem/2**30:.1f}G', file=out)\n",
        "        total += mem\n",
        "    print(f'  TOTAL: {total/2**30:.1f}G', file=out)\n",
        "\n",
        "report_memory_usage(\"4bit\")\n",
        "\n",
        "#max memory allocation (original):\n",
        "#  cuda:0: 5.8G\n",
        "#  TOTAL: 5.8G\n",
        "\n",
        "# max memory allocation (4bit):\n",
        "#  cuda:0: 1.2G\n",
        "#  TOTAL: 1.2G"
      ],
      "metadata": {
        "id": "TztXyKeKXADq",
        "outputId": "6b2d22a6-17b0-42a0-d57e-d599c6cf846f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max memory allocation 4bit:\n",
            "  cuda:0: 1.2G\n",
            "  TOTAL: 1.2G\n"
          ]
        }
      ]
    }
  ]
}