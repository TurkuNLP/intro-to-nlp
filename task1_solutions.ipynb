{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO/flMimf8UOxskr0PBbBFS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/task1_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Lexical ambiguity in a Universal Dependencies dataset\n",
        "\n",
        "* There are many ways to approach this exercise\n",
        "* If your details differ, that's fine\n",
        "* Let's start by grabbing the data"
      ],
      "metadata": {
        "id": "t5y3CSyk07EK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "K8Q_gcRj05nX"
      },
      "outputs": [],
      "source": [
        "# Get the data into colab, wget is fine\n",
        "# URLs I got by clicking around the UD github\n",
        "\n",
        "!wget -nc --quiet https://github.com/UniversalDependencies/UD_Finnish-TDT/raw/master/fi_tdt-ud-train.conllu\n",
        "!wget -nc --quiet https://github.com/UniversalDependencies/UD_English-EWT/raw/master/en_ewt-ud-train.conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finnish verbs\n",
        "\n"
      ],
      "metadata": {
        "id": "9EqhIEQUer-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a direct, quite non-pythonic approach\n",
        "\n",
        "# Let's loop over the lines, skipping empty lines (sentence separators)\n",
        "# and skipping also metadata lines (start with a #)\n",
        "\n",
        "counter={} #{key: verb inflection form, value: the count}\n",
        "\n",
        "with open(\"fi_tdt-ud-train.conllu\") as f:\n",
        "    for line in f:\n",
        "        line=line.rstrip(\"\\n\") #Remember python leaves the newline in the string, in most cases you want to strip it\n",
        "        if not line or line.startswith(\"#\"): #skip empty and metadata lines\n",
        "            continue\n",
        "        #So here we have a real data line\n",
        "        cols=line.split(\"\\t\") #this is a tab-delimited format, so split into columns\n",
        "        ID,FORM,LEMMA,UPOS,XPOS,FEATS,HEAD,DEPREL,DEPS,MISC=cols\n",
        "        if UPOS==\"VERB\":\n",
        "            counter[FEATS]=counter.get(FEATS,0)+1 #add +1 the count of this particular inflection form (FEATS)\n",
        "\n",
        "#Now we have the counts, maybe we want to print some stats\n",
        "print(f\"Total verb inflection forms: {len(counter)}\")\n",
        "#Maybe we want to print the first few forms and their counts\n",
        "#there are so many ways to sort a dict, just google or ask ChatGPT, here is one:\n",
        "forms_sorted=sorted(counter.items(),key=lambda key_val: key_val[1], reverse=True)\n",
        "for form,count in forms_sorted[:10]:\n",
        "    print(f\"Form {form}     ...seen {count} times\")\n"
      ],
      "metadata": {
        "id": "NPaKfY2A10rt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf967560-cbb8-4766-a584-ca8584132da3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total verb inflection forms: 408\n",
            "Form Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin|Voice=Act     ...seen 2801 times\n",
            "Form Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act     ...seen 2517 times\n",
            "Form InfForm=1|Number=Sing|VerbForm=Inf|Voice=Act     ...seen 2460 times\n",
            "Form Case=Nom|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act     ...seen 1256 times\n",
            "Form Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass     ...seen 1030 times\n",
            "Form Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act     ...seen 820 times\n",
            "Form Case=Ill|InfForm=3|Number=Sing|VerbForm=Inf|Voice=Act     ...seen 727 times\n",
            "Form Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin|Voice=Act     ...seen 661 times\n",
            "Form Case=Nom|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Pass     ...seen 648 times\n",
            "Form Connegative=Yes|Mood=Ind|Tense=Pres|VerbForm=Fin     ...seen 586 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is an alternative approach using collections.Counter\n",
        "# The counter takes a sequence and counts its items into a dictionary\n",
        "# It also has an easy way to get the top N most common elements\n",
        "from collections import Counter\n",
        "\n",
        "# Create a generator which yields the verb inflection forms\n",
        "def yield_infl_forms(file_name,target_pos=\"VERB\"):\n",
        "    \"\"\"\n",
        "    Yields the FEATS field.\n",
        "    file_name: name of the file with conllu data\n",
        "    target_pos: the POS we are interested in, everything else ignored\n",
        "    \"\"\"\n",
        "    with open(file_name) as f:\n",
        "        for line in f:\n",
        "            line=line.rstrip(\"\\n\") #this is unnecessary for this exercise, but useful to do in general\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            #So here we have a real data line\n",
        "            cols=line.split(\"\\t\")\n",
        "            ID,FORM,LEMMA,UPOS,XPOS,FEATS,HEAD,DEPREL,DEPS,MISC=cols\n",
        "            if UPOS==target_pos:\n",
        "                yield FEATS #yield one value at a time -> results in a generator\n",
        "\n",
        "#Now we can count the forms with a Counter\n",
        "counter=Counter(yield_infl_forms(\"fi_tdt-ud-train.conllu\"))\n",
        "print(f\"Total verb inflection forms: {len(counter)}\")\n",
        "for form,count in counter.most_common(10):\n",
        "    print(f\"Form {form}     ...seen {count} times\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xQYYogaDpj9",
        "outputId": "a95136b1-6900-4814-a4d7-154b26a08b00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total verb inflection forms: 408\n",
            "Form Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin|Voice=Act     ...seen 2801 times\n",
            "Form Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act     ...seen 2517 times\n",
            "Form InfForm=1|Number=Sing|VerbForm=Inf|Voice=Act     ...seen 2460 times\n",
            "Form Case=Nom|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Act     ...seen 1256 times\n",
            "Form Mood=Ind|Tense=Pres|VerbForm=Fin|Voice=Pass     ...seen 1030 times\n",
            "Form Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act     ...seen 820 times\n",
            "Form Case=Ill|InfForm=3|Number=Sing|VerbForm=Inf|Voice=Act     ...seen 727 times\n",
            "Form Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin|Voice=Act     ...seen 661 times\n",
            "Form Case=Nom|Number=Sing|PartForm=Past|VerbForm=Part|Voice=Pass     ...seen 648 times\n",
            "Form Connegative=Yes|Mood=Ind|Tense=Pres|VerbForm=Fin     ...seen 586 times\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# English noun-verb\n",
        "\n",
        "* Here we might want to make two counters, one for the verbs, one for the nouns\n",
        "* Then we can look at which words (FORM) are shared among these counters\n"
      ],
      "metadata": {
        "id": "DF1IuA95iumy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe I can adopt the solution from above\n",
        "# let's modify it to yield the FORM\n",
        "def yield_forms(file_name,target_pos=\"VERB\"):\n",
        "    with open(file_name) as f:\n",
        "        for line in f:\n",
        "            line=line.rstrip(\"\\n\") #this is unnecessary for this exercise, but useful to do in general\n",
        "            if not line or line.startswith(\"#\"):\n",
        "                continue\n",
        "            #So here we have a real data line\n",
        "            cols=line.split(\"\\t\")\n",
        "            ID,FORM,LEMMA,UPOS,XPOS,FEATS,HEAD,DEPREL,DEPS,MISC=cols\n",
        "            if UPOS==target_pos:\n",
        "                yield FORM\n",
        "file_name=\"en_ewt-ud-train.conllu\"\n",
        "nouns=Counter(yield_forms(file_name,\"NOUN\"))\n",
        "verbs=Counter(yield_forms(file_name,\"VERB\"))\n",
        "\n",
        "# The most straightforward answer is:\n",
        "common= set(nouns) & set(verbs) # & means set intersection\n",
        "print(f\"There are {len(common)} forms seen as both VERB and NOUN in {file_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLg3jRK3Bfr0",
        "outputId": "6a979277-4c36-4dd5-ed18-bd0a6569360e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 738 forms seen as both VERB and NOUN in en_ewt-ud-train.conllu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also go all-out fancy and list the most ambiguous ones\n",
        "# i.e. those which are most balanced between their noun and verb counts\n",
        "# this is definitely going the extra mile\n",
        "\n",
        "noun_verb_ambiguity=[] #let's make a list of [(ambiguity,count_as_verb,count_as_noun,word)]\n",
        "for word,count_as_verb in verbs.items():\n",
        "    count_as_noun=nouns.get(word,0) #get the count as noun, if seen, or 0 otherwise\n",
        "    proportion_as_verb=count_as_verb/(count_as_verb+count_as_noun)\n",
        "    ambiguity=abs(0.5-proportion_as_verb) #the closer to zero, the closer to a 50:50 distribution we are\n",
        "    if count_as_verb>20: #let's try to get at least somehow common cases\n",
        "        noun_verb_ambiguity.append((ambiguity,count_as_verb,count_as_noun,word))\n",
        "\n",
        "noun_verb_ambiguity.sort() #this will sort ascending, so lowest ambiguity comes first\n",
        "for x in noun_verb_ambiguity[:20]: #let's print the 20 most balanced words\n",
        "    print(x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--E5Bf3WlvNR",
        "outputId": "3e7c2cea-f33d-496e-c83e-d062118cc47f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.0, 110, 110, 'work')\n",
            "(0.01677852348993286, 72, 77, 'call')\n",
            "(0.051724137931034475, 26, 32, 'review')\n",
            "(0.05319148936170215, 26, 21, 'return')\n",
            "(0.06470588235294117, 37, 48, 'change')\n",
            "(0.07894736842105265, 22, 16, 'charge')\n",
            "(0.15217391304347827, 30, 16, 'visit')\n",
            "(0.16666666666666663, 34, 17, 'needs')\n",
            "(0.19655172413793098, 101, 44, 'help')\n",
            "(0.24285714285714288, 26, 9, 'move')\n",
            "(0.25, 54, 18, 'love')\n",
            "(0.26, 38, 12, 'show')\n",
            "(0.2692307692307693, 40, 12, 'start')\n",
            "(0.2777777777777778, 35, 10, 'run')\n",
            "(0.28260869565217395, 36, 10, 'means')\n",
            "(0.2954545454545454, 35, 9, 'wait')\n",
            "(0.30000000000000004, 32, 8, 'set')\n",
            "(0.3125, 26, 6, 'walk')\n",
            "(0.3152173913043478, 75, 17, 'look')\n",
            "(0.31818181818181823, 27, 6, 'living')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coverage of ambiguous words\n",
        "\n",
        "This is perhaps a little more involved, but not much. What we can do is build a dictionary `d` which for each word (FORM in the data) maintains a separate counter. This word-specific counter will count all unique analyses (triples of LEMMA-UPOS-FEATS) and how many times these were seen.\n",
        "\n",
        "Once we have our `d`, we can then go over it and sum up the occurrences. If the length of the word-specific counter is 1,\n",
        "this word was ever seen only with one possible analysis and is\n",
        "not ambiguous. If the length is >1, this word is ambiguous.\n"
      ],
      "metadata": {
        "id": "iho9r4yUnQ3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "d={} #{word -> counter of different unique analyses}\n",
        "\n",
        "#uncomment the one you want\n",
        "data=\"en_ewt-ud-train.conllu\"\n",
        "#data=\"fi_tdt-ud-train.conllu\"\n",
        "with open(data) as f:\n",
        "    for line in f:\n",
        "        line=line.rstrip(\"\\n\")\n",
        "        if not line or line[0]==\"#\":\n",
        "            continue\n",
        "        cols=line.split(\"\\t\")\n",
        "        ID,FORM,LEMMA,UPOS,XPOS,FEATS,HEAD,DEPREL,DEPS,MISC=cols\n",
        "        unique_triple=(LEMMA,UPOS,FEATS) #this counts as analysis\n",
        "        #Now, get the counter for this FORM from d\n",
        "        #here as a counter I will use a normal dictionary, not the Counter class above\n",
        "        #.setdefault is very useful, if you don't know it, read up on it\n",
        "        this_form_counter=d.setdefault(FORM,{}) #for every form, we want to maintain a counter for all the triplets it can represent\n",
        "        #+1 for this occurence:\n",
        "        this_form_counter[unique_triple]=this_form_counter.get(unique_triple,0)+1\n",
        "\n",
        "#...and now go over everything we've got\n",
        "uniques=0\n",
        "total=0\n",
        "#We really don't need the \"form\" for anything, but, well, here is how you get it...\n",
        "for form, this_form_counter in d.items():\n",
        "    #sum up the occurrences of this form across all analyses\n",
        "    times_seen=sum(count for triple,count in this_form_counter.items())\n",
        "    if len(this_form_counter)==1: #unique! this word has only one triple seen, is unambiguous\n",
        "        uniques+=times_seen\n",
        "    total+=times_seen\n",
        "\n",
        "print(f\"Percentage of unambiguous words in {data}:\", uniques/total*100,\"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8cA2y0QljxI",
        "outputId": "0698c8fb-c486-409c-e45f-7020ff83d224"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of unambiguous words in en_ewt-ud-train.conllu: 39.49195809426378 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note how the percentage of unique words in the English data is way less than in the Finnish data!"
      ],
      "metadata": {
        "id": "-4XSn328rEXe"
      }
    }
  ]
}
